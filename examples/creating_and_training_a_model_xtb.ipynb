{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 14:48:44.517500: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nopt/slurm/current/lib:\n",
      "2022-02-01 14:48:44.517539: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.4.1\n",
      "nfp 0.3.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import nfp\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"tensorflow {tf.__version__}\")\n",
    "print(f\"nfp {nfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>CAS</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Type</th>\n",
       "      <th>YSI</th>\n",
       "      <th>YSI_err</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>xtbjson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-ethynyl-2,5-dimethylbenzene</td>\n",
       "      <td>74331-70-7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aromatic</td>\n",
       "      <td>512.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>C#Cc1cc(C)ccc1C</td>\n",
       "      <td>519175_3342b2_bd_coord.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-methylindene</td>\n",
       "      <td>2177-47-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aromatic</td>\n",
       "      <td>500.1</td>\n",
       "      <td>19.1</td>\n",
       "      <td>CC1=Cc2ccccc2C1</td>\n",
       "      <td>437628_fd0a51_bd_coord.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azulene</td>\n",
       "      <td>275-51-4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>aromatic</td>\n",
       "      <td>492.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>c1ccc2cccc-2cc1</td>\n",
       "      <td>435907_1c36e8_bd_coord.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-ethynyl-2-methylbenzene</td>\n",
       "      <td>766-47-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aromatic</td>\n",
       "      <td>485.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>C#Cc1ccccc1C</td>\n",
       "      <td>22307_0_bd_coord.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-ethenyl-2,5-dimethylbenzene</td>\n",
       "      <td>2039-89-6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aromatic</td>\n",
       "      <td>469.3</td>\n",
       "      <td>17.8</td>\n",
       "      <td>C=Cc1cc(C)ccc1C</td>\n",
       "      <td>509114_ad5711_bd_coord.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Species         CAS  Ref      Type    YSI  YSI_err  \\\n",
       "0  1-ethynyl-2,5-dimethylbenzene  74331-70-7  1.0  aromatic  512.7     19.6   \n",
       "1                 2-methylindene   2177-47-1  1.0  aromatic  500.1     19.1   \n",
       "2                        azulene    275-51-4  2.0  aromatic  492.3     19.0   \n",
       "3      1-ethynyl-2-methylbenzene    766-47-2  1.0  aromatic  485.0     18.5   \n",
       "4  1-ethenyl-2,5-dimethylbenzene   2039-89-6  1.0  aromatic  469.3     17.8   \n",
       "\n",
       "            SMILES                      xtbjson  \n",
       "0  C#Cc1cc(C)ccc1C  519175_3342b2_bd_coord.json  \n",
       "1  CC1=Cc2ccccc2C1  437628_fd0a51_bd_coord.json  \n",
       "2  c1ccc2cccc-2cc1  435907_1c36e8_bd_coord.json  \n",
       "3     C#Cc1ccccc1C        22307_0_bd_coord.json  \n",
       "4  C=Cc1cc(C)ccc1C  509114_ad5711_bd_coord.json  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the input data, here YSI (10.1016/j.combustflame.2017.12.005)\n",
    "ysi = pd.read_csv('../data/ysi_xtb.csv')\n",
    "ysi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 50, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "valid, test, train = np.split(ysi[['SMILES','xtbjson']].sample(frac=1., random_state=1), [50, 100])\n",
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to featurize the input molecules\n",
    "from nfp.preprocessing.xtb_preprocessor import xTBSmilesPreprocessor\n",
    "from nfp.preprocessing.features import get_ring_size\n",
    "\n",
    "\n",
    "def atom_featurizer(atom):\n",
    "    \"\"\" Return an string representing the atom type\n",
    "    \"\"\"\n",
    "\n",
    "    return str((\n",
    "        atom.GetSymbol(),\n",
    "        atom.GetIsAromatic(),\n",
    "        get_ring_size(atom, max_size=6),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetTotalNumHs(includeNeighbors=True)\n",
    "    ))\n",
    "\n",
    "\n",
    "def bond_featurizer(bond, flipped=False):\n",
    "    \"\"\" Get a similar classification of the bond type.\n",
    "    Flipped indicates which 'direction' the bond edge is pointing. \"\"\"\n",
    "    \n",
    "    if not flipped:\n",
    "        atoms = \"{}-{}\".format(\n",
    "            *tuple((bond.GetBeginAtom().GetSymbol(),\n",
    "                    bond.GetEndAtom().GetSymbol())))\n",
    "    else:\n",
    "        atoms = \"{}-{}\".format(\n",
    "            *tuple((bond.GetEndAtom().GetSymbol(),\n",
    "                    bond.GetBeginAtom().GetSymbol())))\n",
    "    \n",
    "    btype = str(bond.GetBondType())\n",
    "    ring = 'R{}'.format(get_ring_size(bond, max_size=6)) if bond.IsInRing() else ''\n",
    "    \n",
    "    return \" \".join([atoms, btype, ring]).strip()\n",
    "\n",
    "\n",
    "preprocessor = xTBSmilesPreprocessor(atom_features=atom_featurizer, bond_features=bond_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pre-allocating\n",
      "{'unk': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 14:48:58.336917: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-01 14:48:58.337520: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nopt/slurm/current/lib:\n",
      "2022-02-01 14:48:58.337550: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-01 14:48:58.337590: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (el1): /proc/driver/nvidia/version does not exist\n",
      "2022-02-01 14:48:58.338118: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 14:48:58.339728: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after pre-allocating\n",
      "{'unk': 1, \"('C', False, 0, 3, 2)\": 2, \"('C', False, 0, 3, 0)\": 3, \"('C', False, 0, 4, 3)\": 4, \"('C', False, 0, 4, 2)\": 5, \"('C', False, 0, 4, 0)\": 6, \"('H', False, 0, 1, 0)\": 7, \"('O', False, 0, 2, 1)\": 8, \"('O', False, 0, 2, 0)\": 9, \"('O', False, 0, 1, 0)\": 10, \"('C', False, 0, 3, 1)\": 11, \"('C', False, 0, 4, 1)\": 12, \"('C', True, 'max', 3, 0)\": 13, \"('C', True, 'max', 3, 1)\": 14, \"('C', False, 'max', 3, 0)\": 15, \"('C', False, 'max', 4, 2)\": 16, \"('C', False, 'max', 4, 1)\": 17, \"('C', False, 5, 3, 0)\": 18, \"('C', False, 5, 3, 1)\": 19, \"('C', False, 5, 4, 2)\": 20, \"('C', True, 5, 3, 0)\": 21, \"('C', True, 5, 3, 1)\": 22, \"('O', True, 5, 2, 0)\": 23, \"('C', False, 'max', 3, 1)\": 24, \"('C', False, 0, 2, 0)\": 25, \"('C', False, 5, 4, 1)\": 26, \"('C', False, 0, 2, 1)\": 27, \"('O', False, 5, 2, 0)\": 28}\n"
     ]
    }
   ],
   "source": [
    "# Initially, the preprocessor has no data on atom types, so we have to loop over the \n",
    "# training set once to pre-allocate these mappings\n",
    "print(\"before pre-allocating\")\n",
    "print(preprocessor.atom_tokenizer._data)\n",
    "\n",
    "for row in train.index:\n",
    "    smiles = train.at[row,'SMILES']\n",
    "    jsonfile = train.at[row,'xtbjson']\n",
    "    input_dict = preprocessor(smiles, jsonfile='../data/json/'+jsonfile,train=True)\n",
    "    \n",
    "print()\n",
    "print(\"after pre-allocating\")\n",
    "print(preprocessor.atom_tokenizer._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 6, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main input types for a SMILES-based prediction\n",
    "smiles = 'C=C(C)CC(C)(C)C'\n",
    "jsonfile = '../data/json/4921_0_bd_coord.json'\n",
    "\n",
    "# Atom types, as integer classes\n",
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['atom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bond types, as integer classes\n",
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['bond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0,  8],\n",
       "       [ 0,  9],\n",
       "       [ 1,  0],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 2,  1],\n",
       "       [ 2, 10],\n",
       "       [ 2, 11],\n",
       "       [ 2, 12],\n",
       "       [ 3,  1],\n",
       "       [ 3,  4],\n",
       "       [ 3, 13],\n",
       "       [ 3, 14],\n",
       "       [ 4,  3],\n",
       "       [ 4,  5],\n",
       "       [ 4,  6],\n",
       "       [ 4,  7],\n",
       "       [ 5,  4],\n",
       "       [ 5, 15],\n",
       "       [ 5, 16],\n",
       "       [ 5, 17],\n",
       "       [ 6,  4],\n",
       "       [ 6, 18],\n",
       "       [ 6, 19],\n",
       "       [ 6, 20],\n",
       "       [ 7,  4],\n",
       "       [ 7, 21],\n",
       "       [ 7, 22],\n",
       "       [ 7, 23],\n",
       "       [ 8,  0],\n",
       "       [ 9,  0],\n",
       "       [10,  2],\n",
       "       [11,  2],\n",
       "       [12,  2],\n",
       "       [13,  3],\n",
       "       [14,  3],\n",
       "       [15,  5],\n",
       "       [16,  5],\n",
       "       [17,  5],\n",
       "       [18,  6],\n",
       "       [19,  6],\n",
       "       [20,  6],\n",
       "       [21,  7],\n",
       "       [22,  7],\n",
       "       [23,  7]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A connectivity array, where row i indicates bond i connects atom j to atom k\n",
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['connectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1438e-01, -2.1724e-01, -1.1700e-01, -7.1000e-02, -9.4000e-02,\n",
       "         1.0410e+00,  3.0730e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  3.0767e+01,  9.0910e+00],\n",
       "       [ 2.3920e-02, -3.7200e-03, -1.7400e-01, -5.7000e-02, -1.1600e-01,\n",
       "         1.0910e+00,  2.8850e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.7551e+01,  8.6080e+00],\n",
       "       [-9.1840e-02, -2.3884e-01,  4.7000e-02,  2.7000e-02,  3.7000e-02,\n",
       "         1.0080e+00,  3.0840e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.2643e+01,  6.7840e+00],\n",
       "       [-6.6130e-02, -1.6850e-01,  2.6000e-02, -0.0000e+00,  1.3000e-02,\n",
       "         1.0370e+00,  3.0290e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.1883e+01,  6.6580e+00],\n",
       "       [ 4.2450e-02,  5.1000e-03, -1.2000e-02, -2.6000e-02, -1.9000e-02,\n",
       "         1.0810e+00,  2.8770e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  1.9968e+01,  6.3520e+00],\n",
       "       [-9.9180e-02, -2.4876e-01,  1.9000e-02,  2.2000e-02,  2.1000e-02,\n",
       "         1.0150e+00,  3.0840e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.2623e+01,  6.7800e+00],\n",
       "       [-9.9770e-02, -2.4904e-01,  1.9000e-02,  1.5000e-02,  1.7000e-02,\n",
       "         1.0130e+00,  3.0860e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.2641e+01,  6.7830e+00],\n",
       "       [-9.9470e-02, -2.4877e-01,  1.9000e-02,  1.9000e-02,  1.9000e-02,\n",
       "         1.0140e+00,  3.0860e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.2574e+01,  6.7730e+00],\n",
       "       [ 3.0180e-02,  8.5080e-02, -1.3900e-01, -8.0000e-02, -1.1000e-01,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5760e+00,  2.5100e+00],\n",
       "       [ 2.9980e-02,  8.4230e-02, -1.3800e-01, -8.9000e-02, -1.1300e-01,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.6560e+00,  2.5490e+00],\n",
       "       [ 3.7540e-02,  8.9870e-02, -9.0000e-02, -8.3000e-02, -8.7000e-02,\n",
       "         9.6200e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4460e+00,  2.4460e+00],\n",
       "       [ 3.4440e-02,  8.7390e-02, -6.1000e-02, -5.5000e-02, -5.8000e-02,\n",
       "         9.6600e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4910e+00,  2.4680e+00],\n",
       "       [ 3.5130e-02,  8.8600e-02, -8.6000e-02, -7.2000e-02, -7.9000e-02,\n",
       "         9.6500e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.3580e+00,  2.4010e+00],\n",
       "       [ 3.3330e-02,  9.1040e-02, -6.0000e-02, -6.8000e-02, -6.4000e-02,\n",
       "         9.6700e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4720e+00,  2.4590e+00],\n",
       "       [ 2.8670e-02,  8.6550e-02, -6.8000e-02, -6.1000e-02, -6.5000e-02,\n",
       "         9.7100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4920e+00,  2.4690e+00],\n",
       "       [ 3.1100e-02,  8.4260e-02, -3.7000e-02, -6.8000e-02, -5.2000e-02,\n",
       "         9.6900e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5180e+00,  2.4810e+00],\n",
       "       [ 3.0300e-02,  8.3440e-02, -2.1000e-02, -4.2000e-02, -3.2000e-02,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5450e+00,  2.4950e+00],\n",
       "       [ 2.9660e-02,  8.2760e-02, -2.3000e-02, -4.9000e-02, -3.6000e-02,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5530e+00,  2.4990e+00],\n",
       "       [ 3.0120e-02,  8.3360e-02, -4.2000e-02, -5.3000e-02, -4.8000e-02,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5380e+00,  2.4920e+00],\n",
       "       [ 2.9860e-02,  8.3240e-02, -1.8000e-02, -4.0000e-02, -2.9000e-02,\n",
       "         9.7000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5450e+00,  2.4950e+00],\n",
       "       [ 3.1690e-02,  8.6520e-02,  1.2000e-02, -2.8000e-02, -8.0000e-03,\n",
       "         9.6800e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4480e+00,  2.4470e+00],\n",
       "       [ 3.2330e-02,  8.6770e-02,  4.0000e-03, -2.7000e-02, -1.2000e-02,\n",
       "         9.6800e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.4920e+00,  2.4680e+00],\n",
       "       [ 3.0980e-02,  8.4200e-02, -3.9000e-02, -7.1000e-02, -5.5000e-02,\n",
       "         9.6900e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5450e+00,  2.4950e+00],\n",
       "       [ 2.9070e-02,  8.2460e-02, -2.0000e-02, -4.3000e-02, -3.1000e-02,\n",
       "         9.7100e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  2.5610e+00,  2.5030e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['atom_xtb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9268622 , 1.3242947 ],\n",
       "       [0.9755401 , 1.0764849 ],\n",
       "       [0.976035  , 1.0775048 ],\n",
       "       [1.9268622 , 1.3242947 ],\n",
       "       [1.0250528 , 1.4998841 ],\n",
       "       [1.0109937 , 1.5060523 ],\n",
       "       [1.0250528 , 1.4998841 ],\n",
       "       [0.96929526, 1.0907905 ],\n",
       "       [0.9851119 , 1.0871836 ],\n",
       "       [0.9720007 , 1.0873462 ],\n",
       "       [1.0109937 , 1.5060523 ],\n",
       "       [0.9652852 , 1.5472038 ],\n",
       "       [0.9695576 , 1.0925564 ],\n",
       "       [0.9729345 , 1.0923398 ],\n",
       "       [0.9652852 , 1.5472038 ],\n",
       "       [0.9969557 , 1.5319772 ],\n",
       "       [1.0006814 , 1.5286028 ],\n",
       "       [0.9998898 , 1.5291462 ],\n",
       "       [0.9969557 , 1.5319772 ],\n",
       "       [0.98522764, 1.0879004 ],\n",
       "       [0.9861606 , 1.0879096 ],\n",
       "       [0.98609173, 1.0880784 ],\n",
       "       [1.0006814 , 1.5286028 ],\n",
       "       [0.98516583, 1.0881023 ],\n",
       "       [0.9858648 , 1.0878273 ],\n",
       "       [0.982679  , 1.0868458 ],\n",
       "       [0.9998898 , 1.5291462 ],\n",
       "       [0.98415387, 1.0871557 ],\n",
       "       [0.9851873 , 1.0883454 ],\n",
       "       [0.98518836, 1.0881969 ],\n",
       "       [0.9755401 , 1.0764849 ],\n",
       "       [0.976035  , 1.0775048 ],\n",
       "       [0.96929526, 1.0907905 ],\n",
       "       [0.9851119 , 1.0871836 ],\n",
       "       [0.9720007 , 1.0873462 ],\n",
       "       [0.9695576 , 1.0925564 ],\n",
       "       [0.9729345 , 1.0923398 ],\n",
       "       [0.98522764, 1.0879004 ],\n",
       "       [0.9861606 , 1.0879096 ],\n",
       "       [0.98609173, 1.0880784 ],\n",
       "       [0.98516583, 1.0881023 ],\n",
       "       [0.9858648 , 1.0878273 ],\n",
       "       [0.982679  , 1.0868458 ],\n",
       "       [0.98415387, 1.0871557 ],\n",
       "       [0.9851873 , 1.0883454 ],\n",
       "       [0.98518836, 1.0881969 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['bond_xtb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.27425892, -25.68649081, -10.3364    ,  -5.1588    ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(smiles, jsonfile=jsonfile, train=True)['mol_xtb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tf.data pipeline. There's a lot of specifying data types and\n",
    "# expected shapes for tensorflow to pre-allocate the necessary arrays. But \n",
    "# essentially, this is responsible for calling the input constructor, batching \n",
    "# together multiple molecules, and padding the resulting molecules so that all\n",
    "# molecules in the same batch have the same number of atoms (we pad with zeros,\n",
    "# hence why the atom and bond types above start with 1 as the unknown class)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: ((preprocessor(row.SMILES, '../data/json/'+row.xtbjson, train=True), row.YSI)\n",
    "             for i, row in ysi[ysi.SMILES.isin(train.SMILES)].iterrows()),\n",
    "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
    "    .cache().shuffle(buffer_size=200)\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: ((preprocessor(row.SMILES, '../data/json/'+row.xtbjson, train=False), row.YSI)\n",
    "             for i, row in ysi[ysi.SMILES.isin(valid.SMILES)].iterrows()),\n",
    "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
    "    .cache()\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 14:49:06.063810: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-01 14:49:06.067276: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = next(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([None, None, 8]), TensorShape([None, None, 8]), TensorShape([None, None, 2]), TensorShape([None, 8])]\n",
      "[TensorShape([None, None, 8]), TensorShape([None, None, 8]), TensorShape([None, None, 2]), TensorShape([None, 8])]\n",
      "[TensorShape([None, None, 8]), TensorShape([None, None, 8]), TensorShape([None, None, 2]), TensorShape([None, 8])]\n"
     ]
    }
   ],
   "source": [
    "## Define the keras model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Input layers\n",
    "atom = layers.Input(shape=[None], dtype=tf.int64, name='atom')\n",
    "bond = layers.Input(shape=[None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, 2], dtype=tf.int64, name='connectivity')\n",
    "atom_xtb = layers.Input(shape=[None,None], dtype=tf.float64, name=\"atom_xtb\")\n",
    "bond_xtb = layers.Input(shape=[None,None], dtype=tf.float64, name=\"bond_xtb\")\n",
    "\n",
    "num_features = 8  # Controls the size of the model\n",
    "\n",
    "# Convert from a single integer defining the atom state to a vector\n",
    "# of weights associated with that class\n",
    "atom_state = layers.Embedding(preprocessor.atom_classes, num_features,\n",
    "                              name='atom_embedding', mask_zero=True)(atom)\n",
    "\n",
    "# Ditto with the bond state\n",
    "bond_state = layers.Embedding(preprocessor.bond_classes, num_features,\n",
    "                              name='bond_embedding', mask_zero=True)(bond)\n",
    "\n",
    "# Here we use our first nfp layer. This is an attention layer that looks at\n",
    "# the atom and bond states and reduces them to a single, graph-level vector. \n",
    "# mum_heads * units has to be the same dimension as the atom / bond dimension\n",
    "global_state = nfp.GlobalUpdate(units=8, num_heads=1)([atom_state, bond_state, connectivity])\n",
    "\n",
    "for _ in range(3):  # Do the message passing\n",
    "    new_bond_state = nfp.EdgeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    bond_state = layers.Add()([bond_state, new_bond_state])\n",
    "    \n",
    "    new_atom_state = nfp.NodeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    atom_state = layers.Add()([atom_state, new_atom_state])\n",
    "    \n",
    "    new_global_state = nfp.GlobalUpdate(units=8, num_heads=1)(\n",
    "        [atom_state, bond_state, connectivity, global_state]) \n",
    "    global_state = layers.Add()([global_state, new_global_state])\n",
    "\n",
    "    \n",
    "# Since the final prediction is a single, molecule-level property (YSI), we \n",
    "# reduce the last global state to a single prediction.\n",
    "ysi_prediction = layers.Dense(1)(global_state)\n",
    "\n",
    "# Construct the tf.keras model\n",
    "model = tf.keras.Model([atom, bond, connectivity, atom_xtb, bond_xtb], [ysi_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/rlmolecule/svss/envs/tf27_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:592: UserWarning: Input dict contained keys ['bond_indices', 'mol_xtb'] which did not match any model input. They will be ignored by the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 7s 1s/step - loss: 89.6957 - val_loss: 104.1403\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 90.0728 - val_loss: 103.8789\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 87.5763 - val_loss: 103.4945\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 89.2564 - val_loss: 102.8956\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 92.3957 - val_loss: 101.8961\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 91.9479 - val_loss: 100.0637\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 86.0807 - val_loss: 95.5032\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 80.6203 - val_loss: 83.5468\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 68.8767 - val_loss: 78.0933\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 64.4152 - val_loss: 81.0304\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 66.5636 - val_loss: 81.1164\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 70.7887 - val_loss: 77.6578\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 62.3534 - val_loss: 78.6554\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 66.7106 - val_loss: 79.3879\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 66.4846 - val_loss: 78.0953\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 64.5384 - val_loss: 77.2566\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 65.2349 - val_loss: 77.4973\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 63.1854 - val_loss: 77.5039\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 62.0618 - val_loss: 76.8574\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 59.5350 - val_loss: 76.9440\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 60.3808 - val_loss: 76.7679\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 59.3118 - val_loss: 76.6795\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 60.6851 - val_loss: 76.8553\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 64.9050 - val_loss: 77.3786\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 63.0165 - val_loss: 77.6968\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 68.3085 - val_loss: 77.0120\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 60.8650 - val_loss: 75.8382\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 59.2354 - val_loss: 75.5865\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 58.7827 - val_loss: 76.2220\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 61.2479 - val_loss: 75.3012\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 57.6180 - val_loss: 74.2274\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 61.5462 - val_loss: 74.1011\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 59.8214 - val_loss: 73.1256\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 60.0742 - val_loss: 72.2864\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 55.5559 - val_loss: 71.3521\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 55.4260 - val_loss: 69.8589\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 56.1927 - val_loss: 67.8273\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 57.9371 - val_loss: 65.4359\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 53.2332 - val_loss: 62.1518\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 51.6943 - val_loss: 58.1403\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 42.7877 - val_loss: 53.5165\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 43.6861 - val_loss: 48.0897\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 39.0778 - val_loss: 41.2808\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 34.2861 - val_loss: 36.9258\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 30.7535 - val_loss: 32.1651\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 24.4201 - val_loss: 34.2050\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 24.3964 - val_loss: 39.9840\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 26.6450 - val_loss: 36.7720\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 26.0261 - val_loss: 32.2325\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 19.7276 - val_loss: 31.0926\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 20.7063 - val_loss: 30.7724\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 21.2172 - val_loss: 30.6167\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 20.5296 - val_loss: 31.8409\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.6064 - val_loss: 31.7600\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 19.1202 - val_loss: 30.8431\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.8930 - val_loss: 29.1852\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.3020 - val_loss: 29.3147\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 20.2255 - val_loss: 29.3093\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 17.7891 - val_loss: 29.7951\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 19.3988 - val_loss: 31.9735\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 19.0405 - val_loss: 31.2371\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.0995 - val_loss: 28.9266\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 17.5320 - val_loss: 30.0275\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.7064 - val_loss: 34.7176\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 19.7782 - val_loss: 32.0842\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 21.4557 - val_loss: 29.7688\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 18.7506 - val_loss: 32.1488\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 18.3865 - val_loss: 36.3625\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 21.7615 - val_loss: 29.2937\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 15.3927 - val_loss: 33.3881\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.7074 - val_loss: 39.4471\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 22.9105 - val_loss: 34.4473\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 19.7357 - val_loss: 30.1155\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.8764 - val_loss: 29.6672\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.1005 - val_loss: 28.2699\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 16.7261 - val_loss: 28.0326\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.1611 - val_loss: 28.7118\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 18.0240 - val_loss: 29.4046\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.2178 - val_loss: 28.8814\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 15.2737 - val_loss: 28.4953\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.9951 - val_loss: 28.1290\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 17.0696 - val_loss: 27.5496\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.6385 - val_loss: 28.6073\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.5023 - val_loss: 28.5329\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.6316 - val_loss: 29.1235\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.0495 - val_loss: 27.3807\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 16.3476 - val_loss: 26.3223\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.3556 - val_loss: 26.6821\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 14.6263 - val_loss: 29.0494\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.8060 - val_loss: 28.3243\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.4821 - val_loss: 27.3747\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 17.3160 - val_loss: 30.0814\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.6148 - val_loss: 27.0652\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 15.7225 - val_loss: 27.2585\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 16.0970 - val_loss: 26.4869\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 15.3523 - val_loss: 26.0890\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 16.2900 - val_loss: 26.5539\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 13.7786 - val_loss: 28.4712\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 16.6781 - val_loss: 32.7345\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 18.4583 - val_loss: 28.7672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd2fd90850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1E-3))\n",
    "\n",
    "# Fit the model. The first epoch is slower, since it needs to cache\n",
    "# the preprocessed molecule inputs\n",
    "model.fit(train_dataset, validation_data=valid_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we create a test dataset that doesn't assume we know the values for the YSI\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: (preprocessor(row.SMILES, '../data/json/'+row.xtbjson, train=False)\n",
    "             for i, row in test.iterrows()),\n",
    "    output_signature=preprocessor.output_signature)\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.83903667831421"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the predictions on the test set\n",
    "test_predictions = model.predict(test_dataset)\n",
    "test_db_values = ysi.set_index('SMILES').reindex(test.SMILES).YSI.values\n",
    "\n",
    "np.abs(test_db_values - test_predictions.flatten()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf27_gpu",
   "language": "python",
   "name": "tf27_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
